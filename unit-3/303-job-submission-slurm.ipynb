{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edb2ad1c-b2ae-4cd1-99ca-7cbfbc6b6723",
   "metadata": {},
   "source": [
    "# Submitting Python Scripts to an HPC Cluster with SLURM\n",
    "\n",
    "This lecture explains how to submit Python scripts to an HPC cluster using SLURM, both interactively with `salloc` and using batch scripts.\n",
    "\n",
    "## Sample python scripts:\n",
    "\n",
    "You should have 2 scripts:\n",
    "\n",
    "### Script 1 (CPU testing):\n",
    "```bash\n",
    "vim cpu_script.py\n",
    "```\n",
    "\n",
    "```Python\n",
    "# CPU test\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"Running cpu_script.py\")\n",
    "print(f\"Hostname: {os.uname().nodename}\")\n",
    "print(\"Starting CPU task...\")\n",
    "time.sleep(20)  # Sample code for testing\n",
    "print(\"CPU task complete.\")\n",
    "```\n",
    "\n",
    "### Script 2 (GPU testing):\n",
    "\n",
    "- ```nvidia-smi` is a command-line utility provided by NVIDIA that stands for NVIDIA System Management Interface. It can be used to monitor and manage NVIDIA GPU devices.\n",
    "\n",
    "\n",
    "```bash\n",
    "vim gpu_script.py\n",
    "```\n",
    "\n",
    "```Python\n",
    "# GPU test\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "print(\"Running my_gpu_script.py (CUDA Direct)\")\n",
    "print(f\"Hostname: {os.uname().nodename}\")\n",
    "\n",
    "try:\n",
    "    # Run nvidia-smi to check for GPU availability\n",
    "    subprocess.run([\"nvidia-smi\"], check=True)\n",
    "\n",
    "    # Display nvidia-smi\n",
    "    print(\"CUDA device detected.\") \n",
    "    print(\"Running nvidia-smi\")\n",
    "\n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"No CUDA device found. Running on CPU.\")\n",
    "    time.sleep(20) # Sample code for testing\n",
    "\n",
    "print(\"Task complete.\")\n",
    "```\n",
    "\n",
    "\n",
    "First, test them in your laptops. Then, ```scp``` them into the HPC cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf21dc5-aa32-4c84-8e23-1103ab746388",
   "metadata": {},
   "source": [
    "## Method 1: Interactive Job Submission with `salloc`\n",
    "\n",
    "`salloc` allocates resources and provides an interactive shell on a compute node, allowing you to run Python scripts and other commands directly.\n",
    "\n",
    "### Basic `salloc` Usage on CPU partitions:\n",
    "\n",
    "#### Example 1:\n",
    "\n",
    "These SLURM commands allocate 1 hour, 1 CPU core, and 1GB of memory  in the cpu partition.\n",
    "\n",
    "```bash\n",
    "salloc --partition cpu --cpus-per-task=1 --time=1:00:00 --mem=1G\n",
    "```\n",
    "\n",
    "```bash\n",
    "salloc --partition cpu -n 1 --time=1:00:00 --mem=1G\n",
    "```\n",
    "\n",
    "```-n``` typically refers to cores, the exact interpretation can depend on how your cluster's SLURM configuration is set up. \n",
    "\n",
    "\n",
    "#### Script run output:\n",
    "\n",
    "```bash\n",
    "salloc --partition cpu --cpus-per-task=1 --time=1:00:00 --mem=1G\n",
    "\n",
    "ssh dgx-node-0-x\n",
    "\n",
    "conda activate py310\n",
    "\n",
    "python cpu_script.py \n",
    "Running my_cpu_script.py\n",
    "Hostname: dgx-node-0-0.cedia.edu.ec\n",
    "Starting CPU intensive task...\n",
    "CPU task complete.\n",
    "```\n",
    "\n",
    "\n",
    "#### **Example 2: (this is NOT possible in the HPC Cedia cluster)**\n",
    "\n",
    "These SLURM commands allocate 1 hour, 1 CPU core, and 1GB of memory  in the cpu partition.\n",
    "\n",
    "```bash\n",
    "salloc -p cpu -N 2 --ntasks-per-node=16 --mem=32G\n",
    "```\n",
    "\n",
    "Here:\n",
    "\n",
    "```-N 2```: Requests 2 nodes\n",
    "\n",
    "```--ntasks-per-node=16```: Requests 16 tasks (cores) per node. Since you're requesting 2 nodes, this results in a total of 32 cores (16 cores/node * 2 nodes = 32 cores).\n",
    "\n",
    "```--mem=32G:``` Requests 32 gigabytes of memory. This memory will be allocated across the two nodes. Depending on the system configuration, this can mean 16GB per node, or that the system will allocate 32GB total, and the system will handle memory allocation between the two nodes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Basic `salloc` Usage on On GPU partitions:\n",
    "\n",
    "#### Example:\n",
    "\n",
    "```bash\n",
    "salloc -p gpu -n 1 -c 16  --mem=1G --gres=gpu:a100_2g.10gb:1 --time=00:10:00\n",
    "```\n",
    "\n",
    "Here:\n",
    "\n",
    "```-p gpu```: This specifies that the job should be submitted to the \"gpu\" partition (or queue). This means that the resources allocated will be on nodes that have GPUs available.\n",
    "\n",
    "```-n 1```: This requests 1 task. In SLURM, a task typically corresponds to a single process.\n",
    "\n",
    "```-c 16```: This specifies that 16 CPU cores should be allocated per task. In this case, since there is only one task, the total number of CPU cores reserved is 16.\n",
    "\n",
    "```--mem=1G```: This requests 1 gigabyte of memory.\n",
    "\n",
    "```--gres=gpu:a100_2g.10gb:1:``` It requests a generic resource (GRES), specifically a GPU with the name **100_2g.10gb** (A100 GPU), which has **2 GB** of dedicated ram, and **10 GB** of shared ram.\n",
    "\n",
    "```:1```: This specifies that one instance of the requested GPU resource should be allocated.\n",
    "\n",
    "```--time=00:10:00:``` This sets the time limit for the job to 10 minutes (0 hours, 10 minutes, 0 seconds).\n",
    "\n",
    "\n",
    "#### Script run output:\n",
    "\n",
    "```bash\n",
    "ssh dgx-node-0-x\n",
    "\n",
    "conda activate py310\n",
    "\n",
    "python gpu_script.py \n",
    "\n",
    "Running my_gpu_script.py (CUDA Direct)\n",
    "Hostname: dgx-node-0-0.cedia.edu.ec\n",
    "Wed Mar 26 15:37:28 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:07:00.0 Off |                   On |\n",
    "| N/A   29C    P0              44W / 400W |     75MiB / 40960MiB |     N/A      Default |\n",
    "|                                         |                      |              Enabled |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| MIG devices:                                                                          |\n",
    "+------------------+--------------------------------+-----------+-----------------------+\n",
    "| GPU  GI  CI  MIG |                   Memory-Usage |        Vol|      Shared           |\n",
    "|      ID  ID  Dev |                     BAR1-Usage | SM     Unc| CE ENC DEC OFA JPG    |\n",
    "|                  |                                |        ECC|                       |\n",
    "|==================+================================+===========+=======================|\n",
    "|  0    5   0   0  |              25MiB /  9856MiB  | 28      0 |  2   0    1    0    0 |\n",
    "|                  |               0MiB / 16383MiB  |           |                       |\n",
    "+------------------+--------------------------------+-----------+-----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|  No running processes found                                                           |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "CUDA device detected.\n",
    "Running nvidia-smi\n",
    "Task complete.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa327d4-1538-4284-84f9-799db5956eeb",
   "metadata": {},
   "source": [
    "## Method 2: Standalone Job Submission with Scripts: Batch jobs\n",
    "\n",
    "Batch jobs are used for running longer or more complex tasks that don't require direct interaction between the user and the HPC.\n",
    "\n",
    "\n",
    "### Test Batch Script on CPUs:\n",
    "\n",
    "We will create a shell script (e.g., ```job1.sh```) containing your SLURM directives and commands:\n",
    "\n",
    "```bash\n",
    "vim job1.sh\n",
    "```\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=my_job1\n",
    "#SBATCH --partition=cpu\n",
    "#SBATCH --time=00:05:00\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=1G\n",
    "#SBATCH --output=my_job1.out\n",
    "#SBATCH --error=my_job1.err\n",
    "\n",
    "# Your commands go here\n",
    "echo \"Starting job...\"\n",
    "date\n",
    "sleep 60\n",
    "echo \"Job finished.\"\n",
    "date\n",
    "```\n",
    "\n",
    "#### Submit job and check:\n",
    "\n",
    "```bash\n",
    "sbatch my_job1.sh\n",
    "```\n",
    "\n",
    "```bash\n",
    "squeue -u $USER\n",
    "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
    "             39170       cpu   my_job wladimir  R       0:39      1 dgx-node-0-2\n",
    "```\n",
    "\n",
    "```bash\n",
    "ls -ltr my_job.*\n",
    "\n",
    "-rw-rw-r-- 1 wladimir.banda wladimir.banda  0 mar 26 16:15 my_job1.err\n",
    "-rw-rw-r-- 1 wladimir.banda wladimir.banda 88 mar 26 16:16 my_job1.out\n",
    "```\n",
    "\n",
    "\n",
    "### Test Batch Script on GPUs:\n",
    "\n",
    "We will create a shell script (e.g., ```job2.sh```) containing your SLURM directives and commands:\n",
    "\n",
    "```bash\n",
    "vim job2.sh\n",
    "```\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=gpu_job\n",
    "#SBATCH --time=0:05:00\n",
    "#SBATCH --cpus-per-task=8\n",
    "#SBATCH --mem=1G\n",
    "#SBATCH --gres=gpu:a100_2g.10gb:1\n",
    "#SBATCH --partition=gpu\n",
    "#SBATCH --output=gpu_job.out\n",
    "#SBATCH --error=gpu_job.err\n",
    "\n",
    "# Your GPU enabled commands here\n",
    "echo \"Starting GPU job...\"\n",
    "date\n",
    "nvidia-smi\n",
    "sleep 120\n",
    "echo \"GPU Job finished.\"\n",
    "\n",
    "```\n",
    "\n",
    "#### Submit job and check:\n",
    "\n",
    "```bash\n",
    "sbatch my_job2.sh\n",
    "```\n",
    "\n",
    "```bash\n",
    "squeue -u $USER\n",
    "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
    "             39171       gpu  gpu_job wladimir  R       0:29      1 dgx-node-0-0\n",
    "```\n",
    "\n",
    "```bash\n",
    "ls -ltr my_job2.*\n",
    "\n",
    "-rw-rw-r-- 1 wladimir.banda wladimir.banda    0 mar 26 16:22 gpu_job.err\n",
    "-rw-rw-r-- 1 wladimir.banda wladimir.banda 2739 mar 26 16:24 gpu_job.out\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805787f2-41d7-42a7-8402-a3c49bf5eab4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
