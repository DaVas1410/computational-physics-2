{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "964a77a3",
   "metadata": {},
   "source": [
    "# MPI Collective Communication\n",
    "\n",
    "Previously, we used point-to-point communication (i.e. **Send** and **Recv**) to sum the results across all ranks:\n",
    "\n",
    "```python\n",
    "    if my_rank == 0:\n",
    "        world_sum = sum\n",
    "        for i in range( 1, world_size ):\n",
    "            sum_np = np.empty( 1 )\n",
    "            world_comm.Recv( [sum_np, MPI.DOUBLE], source=i, tag=77 )\n",
    "            world_sum += sum_np[0]\n",
    "        average = world_sum / N\n",
    "    else:\n",
    "        sum_np = np.array( [sum] )\n",
    "        world_comm.Send( [sum_np, MPI.DOUBLE], dest=0, tag=77 )\n",
    "```\n",
    "\n",
    "MPI provides many collective communication functions, which automate many processes that can be complicated to write out using only point-to-point communication.\n",
    "\n",
    "### Reduce function:\n",
    "\n",
    "In particular, the **Reduce** function allows us to sum a value across all ranks, without all of the above code. Replace the above with:\n",
    "\n",
    "```python\n",
    "    sum = np.array( [sum] )\n",
    "    world_sum = np.zeros( 1 )\n",
    "    world_comm.Reduce( [sum, MPI.DOUBLE], [world_sum, MPI.DOUBLE], op = MPI.SUM, root = 0 )\n",
    "    average = world_sum / N\n",
    "```\n",
    "\n",
    "The **op** argument lets us specify what operation should be performed on all of the data that is reduced. Setting this argument to **MPI.SUM**, as we do above, causes all of the values to be summed onto the root process. There are many other operations provided by **MPI**, as you can see here:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Note that in addition to enabling us to write simpler-looking code, collective communication operations tend to be faster than what we can achieve by trying to write our own communication operations using point-to-point calls.\n",
    "\n",
    "The code should look like this in **example_mpi10.py**:\n",
    "\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # get basic information about the MPI communicator\n",
    "    world_comm = MPI.COMM_WORLD\n",
    "    world_size = world_comm.Get_size()\n",
    "    my_rank = world_comm.Get_rank()\n",
    "\n",
    "    N = 10000000\n",
    "\n",
    "    # determine the workload of each rank\n",
    "    workloads = [ N // world_size for i in range(world_size) ]\n",
    "    for i in range( N % world_size ):\n",
    "        workloads[i] += 1\n",
    "    my_start = 0\n",
    "    for i in range( my_rank ):\n",
    "        my_start += workloads[i]\n",
    "    my_end = my_start + workloads[my_rank]\n",
    "\n",
    "    # initialize a\n",
    "    start_time = MPI.Wtime()\n",
    "    a = np.ones( workloads[my_rank] )\n",
    "    end_time = MPI.Wtime()\n",
    "    if my_rank == 0:\n",
    "        print(\"Initialize a time: \" + str(end_time-start_time))\n",
    "\n",
    "    # initialize b\n",
    "    start_time = MPI.Wtime()\n",
    "    b = np.zeros( workloads[my_rank] )\n",
    "    for i in range( workloads[my_rank] ):\n",
    "        b[i] = 1.0 + ( i + my_start )\n",
    "    end_time = MPI.Wtime()\n",
    "    if my_rank == 0:\n",
    "        print(\"Initialize b time: \" + str(end_time-start_time))\n",
    "\n",
    "    # add the two arrays\n",
    "    start_time = MPI.Wtime()\n",
    "    for i in range( workloads[my_rank] ):\n",
    "        a[i] = a[i] + b[i]\n",
    "    end_time = MPI.Wtime()\n",
    "    if my_rank == 0:\n",
    "        print(\"Add arrays time: \" + str(end_time-start_time))\n",
    "\n",
    "    # average the result\n",
    "    start_time = MPI.Wtime()\n",
    "    sum = 0.0\n",
    "    for i in range( workloads[my_rank] ):\n",
    "        sum += a[i]\n",
    "    sum = np.array( [sum] )\n",
    "    world_sum = np.zeros( 1 )\n",
    "    world_comm.Reduce( [sum, MPI.DOUBLE], [world_sum, MPI.DOUBLE], op = MPI.SUM, root = 0 )\n",
    "    average = world_sum / N\n",
    "    end_time = MPI.Wtime()\n",
    "    if my_rank == 0:\n",
    "        print(\"Average result time: \" + str(end_time-start_time))\n",
    "        print(\"Average: \" + str(average))\n",
    "```\n",
    "\n",
    "Then, we run the code again:\n",
    "\n",
    "```\n",
    "wladimir$ mpirun -np 2 python example_mpi10.py\n",
    "Initialize a time: 0.031435\n",
    "Initialize b time: 1.088035\n",
    "Add arrays time: 1.8827329999999998\n",
    "Average result time: 1.1045690000000001\n",
    "Average: [5000001.5]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcfa792-9d20-4a39-beaf-8a70739cf02b",
   "metadata": {},
   "source": [
    "## Broadcast (MPI_Bcast):\n",
    "\n",
    "- MPI_Bcast sends the same data from one root process to all other processes in a communicator.\n",
    "\n",
    "- A single piece of data is duplicated and sent to every process.\n",
    "\n",
    "- MPI_Bcast distributes configuration parameters, initial data, or any information that every process needs to have a copy of.\n",
    "\n",
    "### Example on broadcast:\n",
    "\n",
    "```python \n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.rank\n",
    "\n",
    "if rank == 0:\n",
    "    data = {'a':1,'b':2,'c':3}\n",
    "else:\n",
    "    data = None\n",
    "\n",
    "data = comm.bcast(data, root=0)\n",
    "print('rank',rank,data)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe46e2e-e224-4066-af9b-669d08f28bf9",
   "metadata": {},
   "source": [
    "# Scatter(MPI_Scatter):\n",
    "\n",
    "- MPI_Scatter sistributes different chunks of data from one root process to each process in a communicator.\n",
    "\n",
    "- The root process has an array (or a similar data structure), and it divides that array into chunks, sending a different chunk to each process.   \n",
    "\n",
    "- MPI_Scatter divides data for parallel processing, where each process needs to work on a distinct portion of the data.   \n",
    "\n",
    "```python \n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "   data = [(2*x+1)**x for x in range(size)]\n",
    "   print('we will be scattering: ',data)\n",
    "else:\n",
    "   data = None\n",
    "   \n",
    "data = comm.scatter(data, root=0)\n",
    "print('rank '+ str(rank) + ' has data: ' + str(data))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef9c962-d804-427c-80ca-99d18ad1d29b",
   "metadata": {},
   "source": [
    "# Gather (MPI.Gather)\n",
    "\n",
    "- Gather is a collective communication operation used to collect data from all processes in a communicator into a single process (the root process).\n",
    "\n",
    "- Gather is designed to bring together pieces of data from multiple processes into a single, combined dataset on one designated process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf6ed40-a996-4fd9-8915-c15fcc479743",
   "metadata": {},
   "source": [
    "## Example on Scatter and Gather\n",
    "\n",
    "```python\n",
    "# Import the MPI library\n",
    "from mpi4py import MPI  \n",
    "\n",
    "# Create a world communicator\n",
    "comm = MPI.COMM_WORLD \n",
    "\n",
    "# Get the total number of processes and rank IDs in the communicator\n",
    "size = comm.Get_size()  \n",
    "rank = comm.Get_rank()\n",
    "\n",
    "# If the current process is the root process (rank 0)\n",
    "if rank == 0:\n",
    "    \n",
    "    # List of data with (x+1)^x.\n",
    "    data = [(x + 1) ** x for x in range(size)]\n",
    "    \n",
    "    # Print the data that will be scattered\n",
    "    print('we will be scattering:', data)  \n",
    "\n",
    "else:\n",
    "    # Initialize data to None, non-root processes will receive data via scatter\n",
    "    data = None  \n",
    "\n",
    "# Distribute the data list from the root process to all processes, each receiving one element.\n",
    "data = comm.scatter(data, root=0)  \n",
    "\n",
    "# The root process also receives its own data and \n",
    "data += 10  \n",
    "\n",
    "# Print the rank and the modified data on each proces\n",
    "print('rank', rank, 'has data:', data)\n",
    "\n",
    "# Gather the modified data from all processes to the root process\n",
    "newData = comm.gather(data, root=0)  \n",
    "\n",
    "# If the current process is the root process\n",
    "if rank == 0:\n",
    "    # Print the gathered data on the root process\n",
    "    print('master:', newData)  \n",
    "    \n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
