{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wbandabarragan/computational-physics-2/blob/main/unit-3/310_GPU_and_APIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d235efe5-3a59-4320-a1a5-aa0e506f3480",
      "metadata": {
        "id": "d235efe5-3a59-4320-a1a5-aa0e506f3480"
      },
      "source": [
        "# GPUs and APIs\n",
        "\n",
        "# TensorFlow and GPU Parallelization:\n",
        "\n",
        "TensorFlow is designed to take advantage of GPUs (Graphics Processing Units) to accelerate machine learning computations, especially deep learning models that involve large matrix operations.\n",
        "\n",
        "GPU parallelization capabilities and functions:\n",
        "\n",
        "- TensorFlow allows you to explicitly specify which device (CPU or GPU) to use for a particular operation.\n",
        "\n",
        "## GPU abstraction and parallelisation:\n",
        "\n",
        "- TensorFlow abstracts away the complexities of GPU programming, so you don't need to write low-level CUDA or OpenCL code.\n",
        "\n",
        "- TensorFlow handles GPU memory management, kernel execution, and data transfer between CPU and GPU.\n",
        "\n",
        "-  Many TensorFlow operations (like matrix multiplication, convolution, and activation functions) are automatically parallelized on GPUs. TensorFlow automatically manages GPU memory allocation and deallocation.\n",
        "\n",
        "- TensorFlow's runtime optimizes these operations for efficient GPU execution. It uses a memory allocator to efficiently allocate and reuse GPU memory.\n",
        "\n",
        "## Key Functions:\n",
        "\n",
        "- **`tf.device()`** This is to direct operations to specific GPUs. If you don't specify, TensorFlow will attempt to use available GPUs automatically.\n",
        "\n",
        "-  **`tf.config.list_physical_devices('GPU')`:** This function returns a list of all physical GPU devices that are available to TensorFlow. You can use this to check if TensorFlow is detecting your GPUs.\n",
        "\n",
        "- **`tf.device('/GPU:0')`:** This context manager allows you to explicitly place operations on a specific GPU. `/GPU:0` refers to the first GPU, `/GPU:1` to the second, and so on.\n",
        "\n",
        "    \n",
        "## CUDA Integration:\n",
        "\n",
        "- TensorFlow relies on NVIDIA's CUDA and cuDNN libraries for GPU acceleration.\n",
        "\n",
        "- These libraries provide highly optimized routines for deep learning operations.\n",
        "\n",
        "\n",
        "## Conda installation on a laptop or HPC with GPUs:\n",
        "\n",
        "``\n",
        "conda install tensorflow-gpu\n",
        "``\n",
        "\n",
        "## Use on Google Colab:\n",
        "\n",
        "1. Change runtime type to T4 GPU (Go to the Runtime menu).\n",
        "\n",
        "\n",
        "2. Import tensorflow:\n",
        "```Python\n",
        "import tensorflow as tf\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np8NjaYGpY0q",
        "outputId": "edd02085-1acd-42e8-99dd-882fb737f611"
      },
      "id": "Np8NjaYGpY0q",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 15 18:10:51 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8             11W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPU cores:\n",
        "\n",
        "- **CUDA Cores:** The NVIDIA Tesla T4 GPU has 2560 CUDA cores. These are general-purpose parallel processing units.\n",
        "\n",
        "- **Tensor Cores:** The Tesla T4 also features 320 Tensor Cores. These are specialized cores designed to accelerate matrix multiplications, which are fundamental for deep learning workloads."
      ],
      "metadata": {
        "id": "AEDx2Livpmz4"
      },
      "id": "AEDx2Livpmz4"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e3531a49-e3f2-4ce6-8283-df0f5bee10a1",
      "metadata": {
        "id": "e3531a49-e3f2-4ce6-8283-df0f5bee10a1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of GPUs detected\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "print(\"Num GPUs Available:\", len(gpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFFQOM3For4m",
        "outputId": "de88a144-25d2-4b64-95c1-55ffd707fabc"
      },
      "id": "QFFQOM3For4m",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While tf.config.list_physical_devices('GPU') shows you have 1 physical GPU device, that device (the Tesla T4) contains 2560 CUDA cores and 320 Tensor Cores that TensorFlow can utilize for parallel computation."
      ],
      "metadata": {
        "id": "nv656hULpxp2"
      },
      "id": "nv656hULpxp2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Details of each GPU\n",
        "for gpu in gpus:\n",
        "    print(\"GPU Name:\", gpu.name, \" Type:\", gpu.device_type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzNk6Uhio2Me",
        "outputId": "0ad56fd7-3db0-4fb1-b3b7-c17913bb0144"
      },
      "id": "kzNk6Uhio2Me",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Name: /physical_device:GPU:0  Type: GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Boolean indicating GPU availability\n",
        "\n",
        "gpu_available = tf.config.list_physical_devices('GPU')\n",
        "print(\"GPU Available:\", bool(gpu_available))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-n0TOyaqF0s",
        "outputId": "6ae45704-fe95-4adb-f6d2-7943f50ff5f1"
      },
      "id": "5-n0TOyaqF0s",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Whether TensorFlow is built with CUDA\n",
        "cuda_available = tf.test.is_built_with_cuda()\n",
        "print(\"CUDA Available:\", cuda_available)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfAnTMHnqHRj",
        "outputId": "06a6bb2e-0c66-4b7d-b74e-98d22596a1d7"
      },
      "id": "AfAnTMHnqHRj",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usage example:"
      ],
      "metadata": {
        "id": "36wDN2IlqSEi"
      },
      "id": "36wDN2IlqSEi"
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "AkMUhscRqVnx"
      },
      "id": "AkMUhscRqVnx",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the size of the matrices\n",
        "n_points = 4096  # A relatively large matrix\n",
        "\n",
        "# Create two random matrices\n",
        "a = tf.random.normal((n_points, n_points))\n",
        "b = tf.random.normal((n_points, n_points))\n",
        "\n",
        "print(type(a))\n",
        "\n",
        "with tf.device('/GPU:0'):\n",
        "    # Start time stamp\n",
        "    start_gpu = time.time()\n",
        "\n",
        "    # Matrix Multiplication\n",
        "    c_gpu = tf.matmul(a, b)\n",
        "\n",
        "    # End time stamp\n",
        "    end_gpu = time.time()\n",
        "\n",
        "    # Execution time\n",
        "    gpu_time = end_gpu - start_gpu\n",
        "\n",
        "    print(f\"Matrix multiplication on GPU took: {gpu_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RS01gB7dqIkz",
        "outputId": "ea8810d7-1e20-4e3b-bafa-2638121b4fc5"
      },
      "id": "RS01gB7dqIkz",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "Matrix multiplication on GPU took: 0.0004 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matrix multiplication on GPU:\n",
        "\n",
        "- When the ``tf.matmul(a, b)`` operation is placed on the GPU, the backend will automatically distribute the numerous calculations involved in matrix multiplication across the many available CUDA cores on your GPU.\n",
        "\n",
        "- Each core performs a part of the overall computation in parallel, leading to a significant reduction in the total execution time compared to the sequential processing on the CPU."
      ],
      "metadata": {
        "id": "yGHEpQ6lrCJm"
      },
      "id": "yGHEpQ6lrCJm"
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/GPU:0'):\n",
        "    # Start time stamp\n",
        "    start_gpu = time.time()\n",
        "\n",
        "    # Matrix Multiplication\n",
        "    c_gpu = tf.matmul(a, b)\n",
        "\n",
        "    # End time stamp\n",
        "    end_gpu = time.time()\n",
        "\n",
        "    # Execution time\n",
        "    gpu_time = end_gpu - start_gpu\n",
        "\n",
        "    print(f\"Matrix multiplication on GPU took: {gpu_time:.4f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PBicmF-qRMe",
        "outputId": "e2ebd305-3fc2-447c-9868-3c2451eaf3d1"
      },
      "id": "_PBicmF-qRMe",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix multiplication on GPU took: 0.0006 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matrix multiplication on CPU:"
      ],
      "metadata": {
        "id": "sE70ZCoPrHGy"
      },
      "id": "sE70ZCoPrHGy"
    },
    {
      "cell_type": "code",
      "source": [
        "# Start time stamp\n",
        "start_cpu = time.time()\n",
        "\n",
        "# Matrix Multiplication -> We move to NumPy\n",
        "c_cpu = tf.matmul(a.numpy(), b.numpy())\n",
        "#c_cpu = a.numpy() @ b.numpy()\n",
        "\n",
        "# End time stamp\n",
        "end_cpu = time.time()\n",
        "\n",
        "# Execution time\n",
        "cpu_time = end_cpu - start_cpu\n",
        "\n",
        "print(f\"Matrix multiplication on CPU took: {cpu_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cdEIznQqZnm",
        "outputId": "d542b9a2-2727-438f-f173-a1196eaf2ef4"
      },
      "id": "4cdEIznQqZnm",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix multiplication on CPU took: 0.3267 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Speedup:"
      ],
      "metadata": {
        "id": "cBMAev_QrVhf"
      },
      "id": "cBMAev_QrVhf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Ratio of execution times\n",
        "speedup = cpu_time / gpu_time\n",
        "print(f\"Speedup (CPU/GPU): {speedup:.2f}x\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUHDcGigqasK",
        "outputId": "f07e7f1a-6b16-4c7a-dd1a-30f42569a09a"
      },
      "id": "zUHDcGigqasK",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speedup (CPU/GPU): 577.17x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running on GPUs on the HPC-Cedia cluster:\n",
        "\n",
        "- The NVIDIA A100 GPU has 6912 CUDA cores.\n",
        "\n",
        "- It also features 432 third-generation Tensor Cores, which are specialized units designed to accelerate matrix multiplications and deep learning applications\n",
        "\n",
        "\n",
        "### Install:\n",
        "\n",
        "- Activate your environment or create one for GPUs specifically:\n",
        "\n",
        "```\n",
        "conda activate py39\n",
        "```\n",
        "\n",
        "- Install tensorflow with GPU support:\n",
        "\n",
        "```\n",
        "conda install tensorflow-gpu\n",
        "```\n",
        "\n",
        "- Request resources interactively:\n",
        "\n",
        "```\n",
        "salloc -p gpu -n 1 -c 16  --mem=1GB --gres=gpu:a100_2g.10gb:1 --time=00:30:00\n",
        "```"
      ],
      "metadata": {
        "id": "_pzVkR01v5v_"
      },
      "id": "_pzVkR01v5v_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Optional) Application Programming Interfaces (APIs)\n",
        "\n",
        "\n",
        "- An API\n",
        "is a set of rules and protocols that allows different software applications to communicate and exchange data with each other.\n",
        "\n",
        "- An API lists the operations that are available and how to request them (order), without the need to know the intricate details of how the service works internally.\n",
        "\n",
        "A basic C API provides a set of C functions that allow Python to call and utilize specific functionalities implemented in C code. It acts as a simple interface to interact with the underlying C logic.\n",
        "\n",
        "\n",
        "## Example:\n",
        "\n",
        "Say Hi from C using python.\n",
        "\n",
        "\n",
        "### 1. Basic C module with Python C APIs:\n",
        "\n",
        "```bash\n",
        "mkdir C_API_example && cd C_API_example\n",
        "\n",
        "vim hola_modulo.c\n",
        "```\n",
        "\n",
        "\n",
        "```C\n",
        "// Includes the Python.h header file, which provides the Python C API.\n",
        "#include <Python.h>\n",
        "\n",
        "// Includes the standard input/output library for C.\n",
        "#include <stdio.h>\n",
        "\n",
        "// Defining C function that will be accessible from Python with 'self' and 'args' from Python.\n",
        "static PyObject* py_hello(PyObject *self, PyObject *args) {\n",
        "\n",
        "\t// Print a message:\n",
        "    printf(\"Hola Mundo desde el lenguaje C!\\n\");\n",
        "\n",
        "    // No value to be returned. Returns a 'None' object.\n",
        "    Py_RETURN_NONE;\n",
        "}\n",
        "\n",
        "// Method table for the module, mapping Python function to C functions.\n",
        "static PyMethodDef HolaMetodos[] = {\n",
        "\t// py_hello -> A pointer to the C function that implements this Python function.\n",
        "    {\"hola\",  py_hello, METH_NOARGS, \"Print 'Hola Mundo desde el lenguaje C!.\"},\n",
        "    // METH_NOARGS -> the function takes no arguments from Python.\n",
        "    // Marking the end of the array of method definitions.\n",
        "    {NULL, NULL, 0, NULL}\n",
        "};\n",
        "\n",
        "// Module structure: provides metadata about the Python module.\n",
        "static struct PyModuleDef hola_modulo = {\n",
        "\t// Internal members of the module definition structure.\n",
        "    PyModuleDef_HEAD_INIT,\n",
        "    \n",
        "    // Name of module\n",
        "    \"hola_modulo\",\n",
        "    \n",
        "    // Module documentation, in this case NULL/  \n",
        "    NULL,\n",
        "    // -1 so the module keeps state in global variables\n",
        "    -1,\n",
        "    // Pointer to PyMethodDef structures defined earlier\n",
        "    HolaMetodos\n",
        "};\n",
        "\n",
        "// Module initialization function, void for call from Python.\n",
        "PyMODINIT_FUNC PyInit_hola_modulo(void) {\n",
        "\t// Creates and returns the Python module object based on the definition in 'hola_modulo'.\n",
        "    return PyModule_Create(&hola_modulo);\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "### 2. Setup script:\n",
        "\n",
        "```bash\n",
        "vim setup.py\n",
        "```\n",
        "\n",
        "```Python\n",
        "# import tools\n",
        "from setuptools import setup, Extension\n",
        "\n",
        "# For compilation\n",
        "module = Extension('hola_modulo', sources=['hola_modulo.c'])\n",
        "\n",
        "# Setup\n",
        "setup(\n",
        "    name='HolaMundoCModule',\n",
        "    version='0.1.0',\n",
        "    ext_modules=[module]\n",
        ")\n",
        "```\n",
        "\n",
        "### 3. Build interface (inplace for development):\n",
        "\n",
        "```bash\n",
        "python setup.py build_ext --inplace\n",
        "````\n",
        "\n",
        "### 4. Test:\n",
        "\n",
        "```Bash\n",
        "python\n",
        "Python 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:52:10)\n",
        "[Clang 14.0.6 ] on darwin\n",
        "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
        ">>> import hola_modulo\n",
        ">>> hola_modulo.hola()\n",
        "Hola Mundo desde el lenguaje C!\n",
        "```"
      ],
      "metadata": {
        "id": "TaFgkBZE-URV"
      },
      "id": "TaFgkBZE-URV"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_86Tu8mPqcEn"
      },
      "id": "_86Tu8mPqcEn",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}